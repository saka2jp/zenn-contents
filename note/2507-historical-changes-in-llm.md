# Bag-of-Words

トークン（単語・形態素）に分割し、出現回数を並べてベクトル化する手法です。

## Bag-of-Wordsのステップ
1. トークン化
2. 辞書（語彙リスト）の作成
3. ベクトル化

### 1. トークン化

よく出てくる例文として「すもももももももものうち」という例文がありますが、これは、「すもも/も/もも/も/もも/の/うち」という単語に区切ることができます。
これをトークン化と呼びます。

トークン化は言語ごとに難易度・複雑さに差があります。例えば英語であれば、基本的には必ずスペースで区切ることができるため、トークン化はかなり容易です。しかし、日本語のように明確な区切りが存在しない言語はトークン化は一工夫必要です。日本語を形態素解析（トークン化＋品詞分類＋読み方）する代表的なオープンソースとしてMeCabがあります。

### 2. 辞書（語彙リスト）の作成

任意の順序をもつ辞書を作成します。今回のケースだと（すもも, もも, も, の, うち）のようなイメージです。

### 3. ベクトル化

辞書に対して出現回数をカウントします。今回のケースだと (1, 2, 2, 1, 1) となります。
このベクトルを比較することで特徴量が似た文書なのかどうかが分析できます。

## Bag-of-Wordsのデメリット
1. すべての単語が等しい重みであること
2. 文章のニュアンスが失われてしまうこと

1は、どの文書でも頻出しやすい単語、例えば助詞や接続詞の出現回数が増え、膨大なデータをベクトル化したときに結果として似た特徴量になってしやすい問題があります。
2は、例えば「生卵は好きだけど、ゆでたまごは嫌い」、「ゆでたまごは好きだけど、生卵は嫌い」という文章があったときに、意味としては真逆なのに、Bag-of-Wordsの手法だと全く同じベクトル（全く同じ特徴）になってしまうという問題です。

1に関してはTF-IDF、2に関してはn-gramという解決方法が登場しました。


# TF-IDF
`Term Frequency × Inverse Document Frequency` の略で、重み付けの手法です。
「の」「は」「も」などの助詞や接続詞はどの文書でも出現回数が多くなりやすいです。
一方、「すもも」「もも」といった固有名詞や専門用語は特定の文書でしか現れず出現回数が少なくなりやすいです。

このように、TF-IDFは「文書集合の中で珍しい語ほど重くする」という手法です。この手法を用いることでより特徴量を明確にすることができます。

# n-gram

